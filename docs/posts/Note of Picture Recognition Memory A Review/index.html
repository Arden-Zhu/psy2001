<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Hao Zhu">
<meta name="dcterms.date" content="2023-10-16">

<title>Psy2001 Journal - Hao Zhu - Note of Picture Recognition Memory: A Review of Research and Theory</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Psy2001 Journal - Hao Zhu</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Note of Picture Recognition Memory: A Review of Research and Theory</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">note</div>
                <div class="quarto-category">psychology</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Hao Zhu </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 16, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">October 18, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>This is a note written while reading <em>Picture Recognition Memory: A Review of Research and Theory</em> written by W. Howard Levie and Susan N. Hathaway.</p>
<ul>
<li>Experiments have a study phase and a test phase. In the study phase, subjects look at a series of pictures one after the other at a controlled rate. In the test phase, subjects see some of the study pictures mixed in with new pictures (distractors) and are asked to discriminate the study pictures from the distractors</li>
<li>Two kinds of tests are commonly used: forced-choice tests and single-item tests.</li>
</ul>
<section id="pictures-versus-words" class="level1">
<h1>Pictures Versus words</h1>
<ul>
<li>pictures are remembered better than words. (?)</li>
<li>no difference between recognition memory for real objects and pictures of the objects.</li>
</ul>
</section>
<section id="pictures-plus-words" class="level1">
<h1>Pictures Plus Words</h1>
<section id="experimenter-provided-words" class="level2">
<h2 class="anchored" data-anchor-id="experimenter-provided-words">Experimenter-Provided Words</h2>
<ul>
<li>when an added label increases the meaningfulness of a picture, recognition is improved</li>
<li>The authors speculate that the sentences caused subjects to review and further process their mental representation of the picture. This more elaborate representation was then more easily recalled than an unelaborated representation.</li>
<li>when the label described only a portion of a picture, children’s recognition of the described part of the picture was improved, but at the expense of the nondescribed parts.</li>
</ul>
</section>
<section id="subject-generated-words" class="level2">
<h2 class="anchored" data-anchor-id="subject-generated-words">Subject-Generated Words</h2>
<ul>
<li>The verbal descriptions increased recognition of the study pictures, but also increased false recognitions of similar distractors.</li>
</ul>
</section>
<section id="dual-code-versus-single-code-memory-models" class="level2">
<h2 class="anchored" data-anchor-id="dual-code-versus-single-code-memory-models">Dual-Code versus Single-Code Memory Models</h2>
<ul>
<li>The dual-code model postulates the existence of two symbolic systems: A verbal system specialized for processing and storing linguistic information, and a nonverbal system specialized for spatial information and mental imagery.</li>
<li>the dual-code model predicts the results of memory experiments: Pictures are remembered better than concrete words, which in turn are remembered better than abstract words</li>
<li>Several theorists have spoken out against the dual-code model. The initial stages of processing pictures and words may differ, but ultimately both types of information are coded in a common memory store consisting of abstract propositions.</li>
</ul>
</section>
</section>
<section id="pictorial-stimulus-variables" class="level1">
<h1>Pictorial Stimulus Variables</h1>
<section id="distinctiveness" class="level2">
<h2 class="anchored" data-anchor-id="distinctiveness">Distinctiveness</h2>
<ul>
<li>Pictures that are “distinctive” are more memorable than less distinctive pictures. distinctiveness was the underlying dimension relating recognition and aesthetic quality.</li>
</ul>
</section>
<section id="the-role-of-distinctiveness-in-picture-word-memory-models" class="level2">
<h2 class="anchored" data-anchor-id="the-role-of-distinctiveness-in-picture-word-memory-models">The Role of Distinctiveness in Picture-Word Memory Models</h2>
<ul>
<li>The Sensory-Semantic Model suggests that the presentation of a picture involves interpreting and storing two aspects of an item: a sensory code that is concerned with the visual appearance of an item, and a semantic code that concerns the meaning of the item</li>
<li>“generic-specific hypothesis”</li>
<li>frequency theory.</li>
</ul>
</section>
<section id="meaningfulness" class="level2">
<h2 class="anchored" data-anchor-id="meaningfulness">Meaningfulness</h2>
<ul>
<li>Perceivers for whom a panicular type of stimulus is more meaningful (e.g., expert chess players rather than novices) will show superior ability to accurately recognize stimulus patterns (chess board positions) involving that class of stimuli</li>
</ul>
</section>
<section id="visual-richness" class="level2">
<h2 class="anchored" data-anchor-id="visual-richness">Visual Richness</h2>
<ul>
<li>studies of the effects of visual richness on picture recognition have shown mixed results.</li>
</ul>
</section>
<section id="other-pictorial-stimulus-variables" class="level2">
<h2 class="anchored" data-anchor-id="other-pictorial-stimulus-variables">Other Pictorial Stimulus Variables</h2>
<ul>
<li>COMPLEXITY.</li>
<li>Color</li>
<li>Motion, Recognition accuracy was better with the moving pictures.</li>
<li>FIGURE-GROUND SEPARATION. alterations in the figure were detected more easily than alterations in the ground.</li>
<li>LUMINANCE. the brighter the image, the better subjects were able later to recognize the picture.</li>
</ul>
</section>
<section id="memory-for-parts-and-attributes-of-pictures" class="level2">
<h2 class="anchored" data-anchor-id="memory-for-parts-and-attributes-of-pictures">Memory for Parts and Attributes of Pictures</h2>
<ul>
<li>evidence showing that global features are processed prior to local features.</li>
<li>however, found that the order in which global versus local information is processed may depend on factors such as the size of the image and the semantic relationship between key local features and global meaning.</li>
<li>Eye movement research shows that fixations are most likely to fall upon the “informative areas” of pictures.</li>
<li>When, however, study pictures are presented for very brief durations (250 milliseconds), picture recognition is more likely to be based upon holistic information than on specific detail information</li>
<li>Often, subjects’ ability to discriminate study pictures from their mirror-images is near chance</li>
</ul>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">Summary</h2>
<ul>
<li>Studies of pictorial stimulus variables suggest that high distinctiveness, high meaningfulness, motion, good figure-ground separation, and high illumination help subjects remember pictures. Studies of the effects of visual richness, complexity and color have yielded mixed results, whereas a high degree of similarity between pictures in the study set diminishes recognition accuracy.</li>
<li>Subjects are unlikely to remember parts of a picture unless they are unexpected, distinctive, or central to the meaning of the picture.</li>
<li>Among the attributes of pictures, subjects are most likely to retain color information and spatial information.</li>
</ul>
</section>
</section>
<section id="encoding-strategy" class="level1">
<h1>Encoding Strategy</h1>
<section id="level-of-processing" class="level2">
<h2 class="anchored" data-anchor-id="level-of-processing">Level of Processing</h2>
<p>It is predicted that information processed at a semantic level will be remembered better than information processed at the sensory surface level.</p>
</section>
<section id="test-related-encoding" class="level2">
<h2 class="anchored" data-anchor-id="test-related-encoding">Test-Related Encoding</h2>
<p>conceptual encoding was best, schematic next best, and acoustic encoding and the control condition poorest.</p>
</section>
<section id="interference-tasks" class="level2">
<h2 class="anchored" data-anchor-id="interference-tasks">Interference Tasks</h2>
<p>the verbal interference task had a less damaging impact upon the recognition of pictures than words.</p>
</section>
<section id="summary-1" class="level2">
<h2 class="anchored" data-anchor-id="summary-1">Summary</h2>
<p>in general, strategies that lead to deeper or more elaborate encoding improve performance on subsequent recognition tasks.</p>
</section>
</section>
<section id="research-design-factors" class="level1">
<h1>Research Design Factors</h1>
<section id="time-intervals" class="level2">
<h2 class="anchored" data-anchor-id="time-intervals">Time Intervals</h2>
<section id="presentation-duration" class="level3">
<h3 class="anchored" data-anchor-id="presentation-duration">PRESENTATION DURATION</h3>
<ul>
<li>recognition is nearly perfect with a presentation rate of only two seconds.</li>
<li>Presentations lasting only one second produce recognition accuracy in the 80% to 90% range</li>
<li>in experiments in which the task requires encoding detailed aspects of complex pictures, a ceiling effect will likely not occur at a two second presentation duration,</li>
</ul>
</section>
<section id="interstimulus-interval" class="level3">
<h3 class="anchored" data-anchor-id="interstimulus-interval">INTERSTIMULUS INTERVAL</h3>
<ul>
<li>Most subsequent researchers, howevert have found that increasing the blank period between pictures does improve recognition accuracy</li>
</ul>
</section>
<section id="test-delay." class="level3">
<h3 class="anchored" data-anchor-id="test-delay.">TEST DELAY.</h3>
<ul>
<li>Numerous researchers have found that performance is poorer on delayed as compared to immediate tests</li>
</ul>
</section>
</section>
</section>
<section id="factors-related-to-the-form-of-the-dependent-measure" class="level1">
<h1>Factors Related to the Form of the Dependent Measure</h1>
<section id="test-form-forces-choice-versus-single-item-methods." class="level3">
<h3 class="anchored" data-anchor-id="test-form-forces-choice-versus-single-item-methods.">TEST-FORM: FORCES-CHOICE VERSUS SINGLE-ITEM METHODS.</h3>
<ul>
<li>Single-item tests are more difficult than two alternative forced-choice tests</li>
</ul>
</section>
<section id="similarity-of-test-items." class="level3">
<h3 class="anchored" data-anchor-id="similarity-of-test-items.">SIMILARITY OF TEST ITEMS.</h3>
<ul>
<li>Recognition performance can be affected dramatically by varying the degree of similarity between study pictures and distractor pictures.</li>
</ul>
</section>
<section id="study-modality-and-test-modality." class="level3">
<h3 class="anchored" data-anchor-id="study-modality-and-test-modality.">STUDY MODALITY AND TEST MODALITY.</h3>
<ul>
<li><ol type="1">
<li>pictures are better than words as study stimuli, and (2) testing in the same modality is better than testing in the opposite modality</li>
</ol></li>
</ul>
</section>
</section>
<section id="individual-differences" class="level1">
<h1>Individual Differences</h1>
<section id="age-differences" class="level2">
<h2 class="anchored" data-anchor-id="age-differences">Age Differences</h2>
<ul>
<li><p>Although young children’s ability to recognize pictures is very good, numerous researchers have now documented improvement in performance with age</p></li>
<li><p>it may be that young children are less likely to label, and hence dual-code, pictures</p></li>
<li><p>When novel and familiar pictures are shown side by side, infants tend to look more at the novel picture.</p></li>
<li><p>although recognition memory for words declines with age, recognition memory for pictures does not.</p></li>
</ul>
</section>
<section id="style-aptitude-and-ability" class="level2">
<h2 class="anchored" data-anchor-id="style-aptitude-and-ability">Style, Aptitude, and Ability</h2>
</section>
<section id="experience-with-the-class-of-stimuli" class="level2">
<h2 class="anchored" data-anchor-id="experience-with-the-class-of-stimuli">Experience with the Class of Stimuli</h2>
</section>
</section>
<section id="some-closely-related-research-areas" class="level1">
<h1>Some Closely Related Research Areas</h1>
<section id="face-recognition" class="level2">
<h2 class="anchored" data-anchor-id="face-recognition">Face Recognition</h2>
</section>
<section id="eye-witness-research" class="level2">
<h2 class="anchored" data-anchor-id="eye-witness-research">Eye Witness Research</h2>
<ul>
<li>no correlation has been found between subjects’ performance and confidence.</li>
</ul>
</section>
<section id="recognition-of-environmental-scenes" class="level2">
<h2 class="anchored" data-anchor-id="recognition-of-environmental-scenes">Recognition of Environmental Scenes</h2>
</section>
</section>
<section id="final-comment" class="level1">
<h1>Final Comment</h1>
<section id="face-recognition-1" class="level2">
<h2 class="anchored" data-anchor-id="face-recognition-1">Face Recognition</h2>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>